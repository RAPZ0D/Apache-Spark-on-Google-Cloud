{"cells": [{"cell_type": "markdown", "id": "94c78729-271e-419b-b180-f9497c8db30c", "metadata": {}, "source": "### Here I am going to make use of Spark SQL where we will not need the normal spark (scala) dataframe functions to transfrom our dataset and provide an output but we will write those Queries in SQL"}, {"cell_type": "code", "execution_count": 1, "id": "feaed4ca-60ff-4042-a88b-b30a84cd4ad1", "metadata": {"tags": []}, "outputs": [], "source": "from pyspark.sql import SparkSession"}, {"cell_type": "code", "execution_count": 2, "id": "d14a2f8a-8804-463f-9097-6f78da49c7e2", "metadata": {"tags": []}, "outputs": [], "source": "data_uri = \"gs://pysparkbucket-jm/bikeshare.csv\""}, {"cell_type": "code", "execution_count": 3, "id": "47d71f5a-b05d-424d-a2e0-7527ea25aa7b", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/03/29 15:08:47 INFO SparkEnv: Registering MapOutputTracker\n24/03/29 15:08:47 INFO SparkEnv: Registering BlockManagerMaster\n24/03/29 15:08:47 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n24/03/29 15:08:47 INFO SparkEnv: Registering OutputCommitCoordinator\n"}], "source": "spark = SparkSession.builder.appName(\"Read_BikeShare_Data\").getOrCreate()"}, {"cell_type": "code", "execution_count": 4, "id": "e1240b09-f367-43a4-95a9-47138ef61697", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df_bikeshare = spark.read.csv(data_uri, header=True, inferSchema=True)"}, {"cell_type": "code", "execution_count": 5, "id": "c760346c-481e-4b42-bb13-ae73c64f1125", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- datetime: timestamp (nullable = true)\n |-- season: integer (nullable = true)\n |-- holiday: integer (nullable = true)\n |-- workingday: integer (nullable = true)\n |-- weather: integer (nullable = true)\n |-- temp: double (nullable = true)\n |-- atemp: double (nullable = true)\n |-- humidity: integer (nullable = true)\n |-- windspeed: double (nullable = true)\n |-- casual: integer (nullable = true)\n |-- registered: integer (nullable = true)\n |-- count: integer (nullable = true)\n\n"}], "source": "df_bikeshare.printSchema()"}, {"cell_type": "code", "execution_count": 6, "id": "aefee566-41bb-4f6d-9d9a-bfe798a4a4cc", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------------+------+-------+----------+-------+----+------+--------+---------+------+----------+-----+\n|           datetime|season|holiday|workingday|weather|temp| atemp|humidity|windspeed|casual|registered|count|\n+-------------------+------+-------+----------+-------+----+------+--------+---------+------+----------+-----+\n|2011-01-01 00:00:00|     1|      0|         0|      1|9.84|14.395|      81|      0.0|     3|        13|   16|\n|2011-01-01 01:00:00|     1|      0|         0|      1|9.02|13.635|      80|      0.0|     8|        32|   40|\n|2011-01-01 02:00:00|     1|      0|         0|      1|9.02|13.635|      80|      0.0|     5|        27|   32|\n|2011-01-01 03:00:00|     1|      0|         0|      1|9.84|14.395|      75|      0.0|     3|        10|   13|\n|2011-01-01 04:00:00|     1|      0|         0|      1|9.84|14.395|      75|      0.0|     0|         1|    1|\n+-------------------+------+-------+----------+-------+----+------+--------+---------+------+----------+-----+\nonly showing top 5 rows\n\n"}], "source": "df_bikeshare.show(5)"}, {"cell_type": "code", "execution_count": 7, "id": "24441c9d-5c37-4334-adce-436da4bd0bf4", "metadata": {"tags": []}, "outputs": [], "source": "# Create a temporary view of the DataFrame\ndf_bikeshare.createOrReplaceTempView(\"bikeshare_data\")"}, {"cell_type": "code", "execution_count": 8, "id": "b983287d-2879-4e8b-a8a2-52d0051ad575", "metadata": {"tags": []}, "outputs": [], "source": "sql_query = \"\"\"\nSELECT * \nFROM bikeshare_data\nLIMIT 10\n\"\"\""}, {"cell_type": "code", "execution_count": 9, "id": "f886ef7c-dbc8-48b1-8964-64a89b12345a", "metadata": {"tags": []}, "outputs": [], "source": "result = spark.sql(sql_query)"}, {"cell_type": "markdown", "id": "54772fef-ac57-4a20-815a-1e5be09d73e5", "metadata": {}, "source": "Same output as df_bikeshare.show()"}, {"cell_type": "code", "execution_count": 10, "id": "eb3a1c87-ea06-4089-8329-9699f9972060", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------------+------+-------+----------+-------+-----+------+--------+---------+------+----------+-----+\n|           datetime|season|holiday|workingday|weather| temp| atemp|humidity|windspeed|casual|registered|count|\n+-------------------+------+-------+----------+-------+-----+------+--------+---------+------+----------+-----+\n|2011-01-01 00:00:00|     1|      0|         0|      1| 9.84|14.395|      81|      0.0|     3|        13|   16|\n|2011-01-01 01:00:00|     1|      0|         0|      1| 9.02|13.635|      80|      0.0|     8|        32|   40|\n|2011-01-01 02:00:00|     1|      0|         0|      1| 9.02|13.635|      80|      0.0|     5|        27|   32|\n|2011-01-01 03:00:00|     1|      0|         0|      1| 9.84|14.395|      75|      0.0|     3|        10|   13|\n|2011-01-01 04:00:00|     1|      0|         0|      1| 9.84|14.395|      75|      0.0|     0|         1|    1|\n|2011-01-01 05:00:00|     1|      0|         0|      2| 9.84| 12.88|      75|   6.0032|     0|         1|    1|\n|2011-01-01 06:00:00|     1|      0|         0|      1| 9.02|13.635|      80|      0.0|     2|         0|    2|\n|2011-01-01 07:00:00|     1|      0|         0|      1|  8.2| 12.88|      86|      0.0|     1|         2|    3|\n|2011-01-01 08:00:00|     1|      0|         0|      1| 9.84|14.395|      75|      0.0|     1|         7|    8|\n|2011-01-01 09:00:00|     1|      0|         0|      1|13.12|17.425|      76|      0.0|     8|         6|   14|\n+-------------------+------+-------+----------+-------+-----+------+--------+---------+------+----------+-----+\n\n"}], "source": "result.show()"}, {"cell_type": "markdown", "id": "7897e536-3757-4a64-8d46-19043d60a346", "metadata": {}, "source": "Perfroming an operation in pyspark using SQL"}, {"cell_type": "code", "execution_count": 11, "id": "bc6ea4a4-b9ee-4337-9dd3-0d7ccfe0740b", "metadata": {"tags": []}, "outputs": [], "source": "sql_query = \"\"\"\nSELECT season, AVG(temp) AS avg_temperature\nFROM bikeshare_data\nGROUP BY season\n\"\"\""}, {"cell_type": "markdown", "id": "29493d0f-d4d3-4621-b709-7d74479a360d", "metadata": {}, "source": "It calculates the average temperature for each season in the bikeshare_data DataFrame using Spark SQL"}, {"cell_type": "code", "execution_count": 12, "id": "c194dde1-4807-42a7-a39c-8144fc99a0b9", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 4:>                                                          (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+------+------------------+\n|season|   avg_temperature|\n+------+------------------+\n|     1|12.530491437081146|\n|     3|28.789110867178955|\n|     4|  16.6492392099493|\n|     2|22.823483351627882|\n+------+------------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "result = spark.sql(sql_query)\nresult.show()"}, {"cell_type": "markdown", "id": "e18d1054-76c1-47f9-889b-23225a01299b", "metadata": {}, "source": "Creating a Managed Table "}, {"cell_type": "code", "execution_count": 13, "id": "f3209c31-c640-4396-abe4-3261dbbd69c8", "metadata": {"tags": []}, "outputs": [], "source": "table_name = \"managed_table_name\""}, {"cell_type": "code", "execution_count": 15, "id": "9db94caa-101b-43ef-a905-49bfaa29a050", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n24/03/29 15:11:08 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"}], "source": "# Save DataFrame as a managed table with overwrite mode\ndf_bikeshare.write.mode(\"overwrite\").saveAsTable(table_name)"}, {"cell_type": "code", "execution_count": 16, "id": "a2519664-b30e-446b-abb8-1fed1cdef5f1", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------------+------+-------+----------+-------+-----+------+--------+---------+------+----------+-----+\n|           datetime|season|holiday|workingday|weather| temp| atemp|humidity|windspeed|casual|registered|count|\n+-------------------+------+-------+----------+-------+-----+------+--------+---------+------+----------+-----+\n|2011-01-01 00:00:00|     1|      0|         0|      1| 9.84|14.395|      81|      0.0|     3|        13|   16|\n|2011-01-01 01:00:00|     1|      0|         0|      1| 9.02|13.635|      80|      0.0|     8|        32|   40|\n|2011-01-01 02:00:00|     1|      0|         0|      1| 9.02|13.635|      80|      0.0|     5|        27|   32|\n|2011-01-01 03:00:00|     1|      0|         0|      1| 9.84|14.395|      75|      0.0|     3|        10|   13|\n|2011-01-01 04:00:00|     1|      0|         0|      1| 9.84|14.395|      75|      0.0|     0|         1|    1|\n|2011-01-01 05:00:00|     1|      0|         0|      2| 9.84| 12.88|      75|   6.0032|     0|         1|    1|\n|2011-01-01 06:00:00|     1|      0|         0|      1| 9.02|13.635|      80|      0.0|     2|         0|    2|\n|2011-01-01 07:00:00|     1|      0|         0|      1|  8.2| 12.88|      86|      0.0|     1|         2|    3|\n|2011-01-01 08:00:00|     1|      0|         0|      1| 9.84|14.395|      75|      0.0|     1|         7|    8|\n|2011-01-01 09:00:00|     1|      0|         0|      1|13.12|17.425|      76|      0.0|     8|         6|   14|\n|2011-01-01 10:00:00|     1|      0|         0|      1|15.58|19.695|      76|  16.9979|    12|        24|   36|\n|2011-01-01 11:00:00|     1|      0|         0|      1|14.76|16.665|      81|  19.0012|    26|        30|   56|\n|2011-01-01 12:00:00|     1|      0|         0|      1|17.22| 21.21|      77|  19.0012|    29|        55|   84|\n|2011-01-01 13:00:00|     1|      0|         0|      2|18.86|22.725|      72|  19.9995|    47|        47|   94|\n|2011-01-01 14:00:00|     1|      0|         0|      2|18.86|22.725|      72|  19.0012|    35|        71|  106|\n|2011-01-01 15:00:00|     1|      0|         0|      2|18.04| 21.97|      77|  19.9995|    40|        70|  110|\n|2011-01-01 16:00:00|     1|      0|         0|      2|17.22| 21.21|      82|  19.9995|    41|        52|   93|\n|2011-01-01 17:00:00|     1|      0|         0|      2|18.04| 21.97|      82|  19.0012|    15|        52|   67|\n|2011-01-01 18:00:00|     1|      0|         0|      3|17.22| 21.21|      88|  16.9979|     9|        26|   35|\n|2011-01-01 19:00:00|     1|      0|         0|      3|17.22| 21.21|      88|  16.9979|     6|        31|   37|\n+-------------------+------+-------+----------+-------+-----+------+--------+---------+------+----------+-----+\n\n"}], "source": "df_managedtable = spark.sql(f\"SELECT * FROM {table_name} LIMIT 20\")\ndf_managedtable.show()"}, {"cell_type": "markdown", "id": "2f211a5c-1da8-4866-a5f6-5abf108600fa", "metadata": {}, "source": "Creating an EXTERNAL table"}, {"cell_type": "code", "execution_count": 17, "id": "a9afa695-4f8f-4743-964a-ac64cc61b00b", "metadata": {"tags": []}, "outputs": [], "source": "table_name = \"external_table_name\"\nbucket_name = \"pysparkbucket-jm\"\ntable_location = f\"gs://{bucket_name}/test\""}, {"cell_type": "code", "execution_count": 19, "id": "ff14e934-d6b1-4afe-881b-204930a3fe56", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df_bikeshare.write.option(\"path\", table_location).saveAsTable(table_name, format=\"parquet\", mode=\"overwrite\")\n"}, {"cell_type": "markdown", "id": "79cc4f20-989e-4116-bf3f-189fd2ba95dd", "metadata": {}, "source": "List all the tables that you created"}, {"cell_type": "code", "execution_count": 20, "id": "f3df19d1-99e9-4a37-9f39-898a544dc1aa", "metadata": {"tags": []}, "outputs": [], "source": "tables = spark.catalog.listTables()"}, {"cell_type": "code", "execution_count": 21, "id": "4fcb8c9d-1091-4d01-adeb-5a0a0f66fa90", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Tables in Spark session:\nexternal_table_name\nmanaged_table_name\nbikeshare_data\n"}], "source": "print(\"Tables in Spark session:\")\nfor table in tables:\n    print(table.name)"}, {"cell_type": "markdown", "id": "2c3d3ddf-49f6-4ba2-b671-743ffeba09c9", "metadata": {}, "source": "Alternative method"}, {"cell_type": "code", "execution_count": 22, "id": "03bb67e1-5729-4a94-97c8-358a48597bb7", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------+-------------------+-----------+\n|namespace|          tableName|isTemporary|\n+---------+-------------------+-----------+\n|  default|external_table_name|      false|\n|  default| managed_table_name|      false|\n|         |     bikeshare_data|       true|\n+---------+-------------------+-----------+\n\n"}], "source": "spark.sql(\"SHOW TABLES\").show()"}, {"cell_type": "markdown", "id": "c25a4835-0cb7-4906-b9ee-0ed34978f218", "metadata": {}, "source": "Dropping a table "}, {"cell_type": "code", "execution_count": 23, "id": "d4bddf92-8df2-445f-bfca-aa62e26ecc2f", "metadata": {"tags": []}, "outputs": [{"data": {"text/plain": "DataFrame[]"}, "execution_count": 23, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sql(\"DROP TABLE managed_table_name\")"}, {"cell_type": "code", "execution_count": 24, "id": "bce74edc-57ea-4c8f-935d-08fc408944eb", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------+-------------------+-----------+\n|namespace|          tableName|isTemporary|\n+---------+-------------------+-----------+\n|  default|external_table_name|      false|\n|         |     bikeshare_data|       true|\n+---------+-------------------+-----------+\n\n"}], "source": "spark.sql(\"SHOW TABLES\").show()"}, {"cell_type": "code", "execution_count": null, "id": "6c285659-f9a3-4930-99f6-95c4049e91ee", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}